<!DOCTYPE html>

<html lang="it" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Documentazione PEAN docs</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=c67112c0"></script>
    <script src="_static/doctools.js?v=fd6eb6e6"></script>
    <script src="_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="_static/translations.js?v=45930005"></script>
    <link rel="index" title="Indice" href="genindex.html" />
    <link rel="search" title="Cerca" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="documentazione-pean">
<h1>Documentazione PEAN<a class="headerlink" href="#documentazione-pean" title="Link to this heading">¶</a></h1>
<div class="toctree-wrapper compound">
<span id="document-doc_contents/pretrain"></span><section id="pretraining">
<h2>Pretraining<a class="headerlink" href="#pretraining" title="Link to this heading">¶</a></h2>
<p>Nel pretrain viene allenato il modello ad “interpretare” i byte cifrati di un pacchetto. Questa capacità interpretativa verrà riutilizzata negli step successivi dell’allenamento complessivo. L’allenamento avviene attraverso il <strong>Masked Language Modeling</strong> (MLM), dove essenzialmente verrà data una sequenza di bytes al modello, alcuni dei quali saranno “mascherati” e l’obiettivo del transformer sarà di ricostruirli.</p>
<dl class="py function">
<dt class="sig sig-object py" id="train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedModel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizer</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#train" title="Link to this definition">¶</a></dt>
<dd><p>Questa è la funzione principale per allenare il modello. Questo è un link ad essa <a class="reference internal" href="#train" title="train"><code class="xref py py-func docutils literal notranslate"><span class="pre">train()</span></code></a></p>
</dd></dl>

<section id="mascherare-i-bytes">
<h3>Mascherare i bytes<a class="headerlink" href="#mascherare-i-bytes" title="Link to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="mask_tokens">
<span class="sig-name descname"><span class="pre">mask_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#mask_tokens" title="Link to this definition">¶</a></dt>
<dd><p>Questa è la funzione usata per mascherare i tokens subit prima di darli in input al modello.</p>
</dd></dl>

</section>
<section id="padding">
<h3>Padding<a class="headerlink" href="#padding" title="Link to this heading">¶</a></h3>
</section>
<section id="dataset">
<h3>Dataset<a class="headerlink" href="#dataset" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="TextDataset">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">TextDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">Dataset</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#TextDataset" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</section>
</section>
<span id="document-doc_contents/configuration"></span><section id="argomenti-di-configurazione">
<h2>Argomenti di configurazione<a class="headerlink" href="#argomenti-di-configurazione" title="Link to this heading">¶</a></h2>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>usage: main.py [-h] [--pad_num PAD_NUM] [--pad_len PAD_LEN]
               [--pad_len_seq PAD_LEN_SEQ] [--emb EMB]
               [--device DEVICE] [--load LOAD]
               [--batch BATCH] [--feature FEATURE]
               [--method METHOD] [--embway EMBWAY]
               [--imploss IMPLOSS] [--lr LR]
               [--length_emb_size LENGTH_EMB_SIZE]
               [--lenhidden LENHIDDEN]
               [--embhidden EMBHIDDEN] [--seed SEED]
               [--trf_heads TRF_HEADS]
               [--trf_layers TRF_LAYERS] [--mode MODE]
               [--k K] [--epoch EPOCH]

Traffic Classification

options:
  -h, --help            show this help message and exit
  --pad_num PAD_NUM     the padding size of packet num
  --pad_len PAD_LEN     the padding size(length) of each
                        packet
  --pad_len_seq PAD_LEN_SEQ
                        the padding size of packet length
                        sequence
  --emb EMB             the emb size of bytes
  --device DEVICE       the training device
  --load LOAD           whether train on previous model
  --batch BATCH         batch_size
  --feature FEATURE     length / raw / ensemble
  --method METHOD       lstm / trf (Sequential Layer)
  --embway EMBWAY       random / pretrain (for raw)
  --imploss IMPLOSS     whether to use improved loss
  --lr LR               learning rate
  --length_emb_size LENGTH_EMB_SIZE
                        len emb size
  --lenhidden LENHIDDEN
                        len hidden size
  --embhidden EMBHIDDEN
                        emb hidden size
  --seed SEED           random seed
  --trf_heads TRF_HEADS
                        transformers heads number
  --trf_layers TRF_LAYERS
                        transformers layers
  --mode MODE           train/test
  --k K                 k fold validation
  --epoch EPOCH         epoch
</pre></div>
</div>
</section>
<span id="document-doc_contents/how_it_works"></span><section id="fasi-del-modello">
<h2>Fasi del modello<a class="headerlink" href="#fasi-del-modello" title="Link to this heading">¶</a></h2>
<p>Qui vengono spiegate nel dettaglio le diverse fasi del modello</p>
<section id="pretrain">
<h3>Pretrain<a class="headerlink" href="#pretrain" title="Link to this heading">¶</a></h3>
<p>Nel pretrain essenzialmente alleniamo il modello a creare associazioni tra i byte per poi concentrare queste “informazioni” all’interno di un token (vettore) speciale.</p>
</section>
<section id="packet-transformer-encoder">
<h3>Packet transformer encoder<a class="headerlink" href="#packet-transformer-encoder" title="Link to this heading">¶</a></h3>
<p>Dopo il pretrain il modello dovrebbe aver imparato ad interpretare i bytes tra di loro. Ad ogni pacchetto aggiungiamo un token speciale [PACKET] in cui il transformer ipoteticamente concentra tutte le correlazioni trovate tra pacchetti. Useremo questi embedding “concentrati” negli step successivi.</p>
</section>
<section id="flow-transformer-encoder">
<h3>Flow transformer encoder<a class="headerlink" href="#flow-transformer-encoder" title="Link to this heading">¶</a></h3>
<p>Ottenuti questi vettori rappresentanti gli interi pacchetti,
ripassiamo per un transformer per creare correlazioni tra di loro. Ogni vettore associato ad un pacchetto otterrà una nuova rappresentazione che include le informazioni dei pacchetti circostanti.</p>
</section>
<section id="supplement-layer-lunghezze">
<h3>Supplement layer (lunghezze)<a class="headerlink" href="#supplement-layer-lunghezze" title="Link to this heading">¶</a></h3>
<p>Per ottenere gli embedding dei pacchetti, dobbiamo troncare o allungare il pacchetto alla lunghezza desiderata dal modello transformer. Da quel punto in poi (PTE, FTE) i pacchetti trasformati avranno tutti la stessa lunghezza. Per non perdere quell’informazione, prima di fare passare per il transformer estraiamo la lunghezza di tutti i pacchetti. Si crea dunque una sequenze di queste lunghezze che viene data ad un secondo tipo di modello chiamato LSTM (long short term memory), in breve adatto ad individuare relazioni temporali all’interno di sequenze.</p>
</section>
<section id="concatenazione-e-classificazione">
<h3>Concatenazione e classificazione<a class="headerlink" href="#concatenazione-e-classificazione" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>Alla fine avremo dunque:</dt><dd><ul class="simple">
<li><p>Una rappresentazione dei pacchetti di un flow di traffico   trasformata per essere utile alla classificazione.</p></li>
<li><p>Una rappresentazione delle lunghezze dei pacchetti, anche   trasformata per esprimere correlazioni tra le lunghezze.</p></li>
</ul>
</dd>
</dl>
<p>Queste due rappresentazioni vengono concatenate e data in input ad una classica rete neurale che predice il tipo di applicazione.</p>
</section>
</section>
</div>
<section id="come-funziona-il-modello">
<h2>Come funziona il modello<a class="headerlink" href="#come-funziona-il-modello" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>Il modello è diviso in 4 fasi diverse, che sono in ordine:</dt><dd><ul class="simple">
<li><p>Pretrain</p></li>
<li><p>Packet tranfsormer encoder (PTE)</p></li>
<li><p>Flow transformer encoder (FTE)</p></li>
<li><p>Supplement layer (lunghezze pacchetti)</p></li>
<li><p>Concatenazione e classificazione</p></li>
</ul>
</dd>
</dl>
<p>Ogni step è spiegato nel dettaglio in <a class="reference internal" href="#document-doc_contents/how_it_works"><span class="doc">Fasi del modello</span></a>.</p>
</section>
<section id="come-vengono-passati-i-dati-al-modello">
<h2>Come vengono passati i dati al modello<a class="headerlink" href="#come-vengono-passati-i-dati-al-modello" title="Link to this heading">¶</a></h2>
<p>Il dataset che il modello si attende è già diviso in traffici corrispondenti alla stessa applicazione ovvero: ogni sample estratto dal dataset contiene 10 pacchetti (ognuno troncato per avere 400 bytes) dello stesso tipo di traffico.</p>
<p>Questo lavoro di catalogazione non è presente all’interno del codice fornito dagli autori.</p>
</section>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Link to this heading">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Indice</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Cerca</span></a></p></li>
</ul>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="#">PEAN docs</a></h1>








<h3>Navigazione</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="#document-doc_contents/pretrain">Pretraining</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-doc_contents/configuration">Argomenti di configurazione</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-doc_contents/how_it_works">Fasi del modello</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Luca Mautino.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 9.0.4</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
    </div>

    

    
  </body>
</html>