<!DOCTYPE html>

<html lang="it" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Pretraining &#8212; Documentazione PEAN docs  </title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <script src="../_static/documentation_options.js?v=3fd72285"></script>
    <script src="../_static/doctools.js?v=fd6eb6e6"></script>
    <script src="../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="../_static/translations.js?v=45930005"></script>
    <link rel="index" title="Indice" href="../genindex.html" />
    <link rel="search" title="Cerca" href="../search.html" />
    <link rel="next" title="Argomenti di configurazione" href="configuration.html" />
    <link rel="prev" title="Documentazione PEAN" href="../index.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="pretraining">
<h1>Pretraining<a class="headerlink" href="#pretraining" title="Link to this heading">¶</a></h1>
<p>Nel pretrain viene allenato il modello ad “interpretare” i byte cifrati di un pacchetto. Questa capacità interpretativa verrà riutilizzata negli step successivi dell’allenamento complessivo. L’allenamento avviene attraverso il <strong>Masked Language Modeling</strong> (MLM), dove essenzialmente verrà data una sequenza di bytes al modello, alcuni dei quali saranno “mascherati” e l’obiettivo del transformer sarà di ricostruirli.</p>
<dl class="py function">
<dt class="sig sig-object py" id="train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedModel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizer</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#train" title="Link to this definition">¶</a></dt>
<dd><p>Questa è la funzione principale per allenare il modello. Questo è un link ad essa <a class="reference internal" href="#train" title="train"><code class="xref py py-func docutils literal notranslate"><span class="pre">train()</span></code></a></p>
</dd></dl>

<section id="mascherare-i-bytes">
<h2>Mascherare i bytes<a class="headerlink" href="#mascherare-i-bytes" title="Link to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="mask_tokens">
<span class="sig-name descname"><span class="pre">mask_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#mask_tokens" title="Link to this definition">¶</a></dt>
<dd><p>Questa è la funzione usata per mascherare i tokens subit prima di darli in input al modello.</p>
</dd></dl>

</section>
<section id="padding">
<h2>Padding<a class="headerlink" href="#padding" title="Link to this heading">¶</a></h2>
</section>
<section id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Link to this heading">¶</a></h2>
<p>Vedi l’argomento <a class="reference internal" href="configuration.html#block_size" title="block_size"><code class="xref py py-data docutils literal notranslate"><span class="pre">block_size</span></code></a>.</p>
<dl class="py class">
<dt class="sig sig-object py" id="TextDataset">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">TextDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">Dataset</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#TextDataset" title="Link to this definition">¶</a></dt>
<dd><p>Carica il dataset dal filepath specificato attraverso pickle. Si aspetta un dataset già tokenizzato. Se non esiste già, crea una cache dove vengono salvati gli id dei tokens, ottimizzando le performance.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="LineByLineTextDataset">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">LineByLineTextDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">Dataset</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#LineByLineTextDataset" title="Link to this definition">¶</a></dt>
<dd><p>Apre il filepath specificato da dataset e tratta ogni riga del file di testo indicato come un flow. A differenza di <a class="reference internal" href="#TextDataset" title="TextDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextDataset</span></code></a> non salva in una cache il testo già tokenizzato.</p>
</dd></dl>

</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">PEAN docs</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Vai" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigazione</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Pretraining</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#train"><code class="docutils literal notranslate"><span class="pre">train()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#mascherare-i-bytes">Mascherare i bytes</a></li>
<li class="toctree-l2"><a class="reference internal" href="#padding">Padding</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dataset">Dataset</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Argomenti di configurazione</a></li>
<li class="toctree-l1"><a class="reference internal" href="how_it_works.html">Fasi del modello</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="../index.html" title="capitolo precedente">Documentazione PEAN</a></li>
      <li>Next: <a href="configuration.html" title="capitolo successivo">Argomenti di configurazione</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Luca Mautino.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 9.0.4</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/doc_contents/pretrain.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>