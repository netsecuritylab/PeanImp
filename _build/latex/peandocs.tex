%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,italian]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
\newdimen\sphinxremdimen\sphinxremdimen = 10pt
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Sonny]{fncychap}
\ChNameVar{\Large\normalfont\sffamily}
\ChTitleVar{\Large\normalfont\sffamily}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsitalian{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{PEAN docs}
\date{09 dic 2025}
\release{}
\author{Luca Mautino}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxstepscope


\chapter{Pretraining}
\label{\detokenize{doc_contents/pretrain:pretraining}}\label{\detokenize{doc_contents/pretrain::doc}}
\sphinxAtStartPar
Nel pretrain viene allenato il modello ad “interpretare” i byte cifrati di un pacchetto. Questa capacità interpretativa verrà riutilizzata negli step successivi dell’allenamento complessivo. L’allenamento avviene attraverso il \sphinxstylestrong{Masked Language Modeling} (MLM), dove essenzialmente verrà data una sequenza di bytes al modello, alcuni dei quali saranno “mascherati” e l’obiettivo del transformer sarà di ricostruirli.
\index{built\sphinxhyphen{}in function@\spxentry{built\sphinxhyphen{}in function}!train()@\spxentry{train()}}\index{train()@\spxentry{train()}!built\sphinxhyphen{}in function@\spxentry{built\sphinxhyphen{}in function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{doc_contents/pretrain:train}}
\pysigstartsignatures
\pysiglinewithargsret
{\sphinxbfcode{\sphinxupquote{train}}}
{\sphinxparam{\DUrole{n}{args}}\sphinxparamcomma \sphinxparam{\DUrole{n}{train\_dataset}}\sphinxparamcomma \sphinxparam{\DUrole{n}{model}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{PreTrainedModel}}\sphinxparamcomma \sphinxparam{\DUrole{n}{tokenizer}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{PreTrainedTokenizer}}}
{{ $\rightarrow$ Tuple\DUrole{p}{{[}}int\DUrole{p}{,}\DUrole{w}{ }float\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Questa è la funzione principale per allenare il modello. Questo è un link ad essa {\hyperref[\detokenize{doc_contents/pretrain:train}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{train()}}}}}

\end{fulllineitems}



\section{Mascherare i bytes}
\label{\detokenize{doc_contents/pretrain:mascherare-i-bytes}}\index{built\sphinxhyphen{}in function@\spxentry{built\sphinxhyphen{}in function}!mask\_tokens()@\spxentry{mask\_tokens()}}\index{mask\_tokens()@\spxentry{mask\_tokens()}!built\sphinxhyphen{}in function@\spxentry{built\sphinxhyphen{}in function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{doc_contents/pretrain:mask_tokens}}
\pysigstartsignatures
\pysiglinewithargsret
{\sphinxbfcode{\sphinxupquote{mask\_tokens}}}
{\sphinxparam{\DUrole{n}{inputs}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{torch.Tensor}}\sphinxparamcomma \sphinxparam{\DUrole{n}{tokenizer}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{PreTrainedTokenizer}}\sphinxparamcomma \sphinxparam{\DUrole{n}{args}}}
{{ $\rightarrow$ Tuple\DUrole{p}{{[}}torch.Tensor\DUrole{p}{,}\DUrole{w}{ }torch.Tensor\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Questa è la funzione usata per mascherare i tokens subit prima di darli in input al modello.

\end{fulllineitems}



\section{Padding}
\label{\detokenize{doc_contents/pretrain:padding}}
\sphinxstepscope


\chapter{Argomenti di configurazione}
\label{\detokenize{doc_contents/configuration:argomenti-di-configurazione}}\label{\detokenize{doc_contents/configuration::doc}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
usage: main.py [\PYGZhy{}h] [\PYGZhy{}\PYGZhy{}pad\PYGZus{}num PAD\PYGZus{}NUM] [\PYGZhy{}\PYGZhy{}pad\PYGZus{}len PAD\PYGZus{}LEN]
               [\PYGZhy{}\PYGZhy{}pad\PYGZus{}len\PYGZus{}seq PAD\PYGZus{}LEN\PYGZus{}SEQ] [\PYGZhy{}\PYGZhy{}emb EMB]
               [\PYGZhy{}\PYGZhy{}device DEVICE] [\PYGZhy{}\PYGZhy{}load LOAD]
               [\PYGZhy{}\PYGZhy{}batch BATCH] [\PYGZhy{}\PYGZhy{}feature FEATURE]
               [\PYGZhy{}\PYGZhy{}method METHOD] [\PYGZhy{}\PYGZhy{}embway EMBWAY]
               [\PYGZhy{}\PYGZhy{}imploss IMPLOSS] [\PYGZhy{}\PYGZhy{}lr LR]
               [\PYGZhy{}\PYGZhy{}length\PYGZus{}emb\PYGZus{}size LENGTH\PYGZus{}EMB\PYGZus{}SIZE]
               [\PYGZhy{}\PYGZhy{}lenhidden LENHIDDEN]
               [\PYGZhy{}\PYGZhy{}embhidden EMBHIDDEN] [\PYGZhy{}\PYGZhy{}seed SEED]
               [\PYGZhy{}\PYGZhy{}trf\PYGZus{}heads TRF\PYGZus{}HEADS]
               [\PYGZhy{}\PYGZhy{}trf\PYGZus{}layers TRF\PYGZus{}LAYERS] [\PYGZhy{}\PYGZhy{}mode MODE]
               [\PYGZhy{}\PYGZhy{}k K] [\PYGZhy{}\PYGZhy{}epoch EPOCH]

Traffic Classification

options:
  \PYGZhy{}h, \PYGZhy{}\PYGZhy{}help            show this help message and exit
  \PYGZhy{}\PYGZhy{}pad\PYGZus{}num PAD\PYGZus{}NUM     the padding size of packet num
  \PYGZhy{}\PYGZhy{}pad\PYGZus{}len PAD\PYGZus{}LEN     the padding size(length) of each
                        packet
  \PYGZhy{}\PYGZhy{}pad\PYGZus{}len\PYGZus{}seq PAD\PYGZus{}LEN\PYGZus{}SEQ
                        the padding size of packet length
                        sequence
  \PYGZhy{}\PYGZhy{}emb EMB             the emb size of bytes
  \PYGZhy{}\PYGZhy{}device DEVICE       the training device
  \PYGZhy{}\PYGZhy{}load LOAD           whether train on previous model
  \PYGZhy{}\PYGZhy{}batch BATCH         batch\PYGZus{}size
  \PYGZhy{}\PYGZhy{}feature FEATURE     length / raw / ensemble
  \PYGZhy{}\PYGZhy{}method METHOD       lstm / trf (Sequential Layer)
  \PYGZhy{}\PYGZhy{}embway EMBWAY       random / pretrain (for raw)
  \PYGZhy{}\PYGZhy{}imploss IMPLOSS     whether to use improved loss
  \PYGZhy{}\PYGZhy{}lr LR               learning rate
  \PYGZhy{}\PYGZhy{}length\PYGZus{}emb\PYGZus{}size LENGTH\PYGZus{}EMB\PYGZus{}SIZE
                        len emb size
  \PYGZhy{}\PYGZhy{}lenhidden LENHIDDEN
                        len hidden size
  \PYGZhy{}\PYGZhy{}embhidden EMBHIDDEN
                        emb hidden size
  \PYGZhy{}\PYGZhy{}seed SEED           random seed
  \PYGZhy{}\PYGZhy{}trf\PYGZus{}heads TRF\PYGZus{}HEADS
                        transformers heads number
  \PYGZhy{}\PYGZhy{}trf\PYGZus{}layers TRF\PYGZus{}LAYERS
                        transformers layers
  \PYGZhy{}\PYGZhy{}mode MODE           train/test
  \PYGZhy{}\PYGZhy{}k K                 k fold validation
  \PYGZhy{}\PYGZhy{}epoch EPOCH         epoch
\end{sphinxVerbatim}

\sphinxstepscope


\chapter{Fasi del modello}
\label{\detokenize{doc_contents/how_it_works:fasi-del-modello}}\label{\detokenize{doc_contents/how_it_works::doc}}
\sphinxAtStartPar
Qui vengono spiegate nel dettaglio le diverse fasi del modello


\section{Pretrain}
\label{\detokenize{doc_contents/how_it_works:pretrain}}
\sphinxAtStartPar
Nel pretrain essenzialmente alleniamo il modello a creare associazioni tra i byte per poi concentrare queste “informazioni” all’interno di un token (vettore) speciale.


\section{Packet transformer encoder}
\label{\detokenize{doc_contents/how_it_works:packet-transformer-encoder}}
\sphinxAtStartPar
Dopo il pretrain il modello dovrebbe aver imparato ad interpretare i bytes tra di loro. Ad ogni pacchetto aggiungiamo un token speciale {[}PACKET{]} in cui il transformer ipoteticamente concentra tutte le correlazioni trovate tra pacchetti. Useremo questi embedding “concentrati” negli step successivi.


\section{Flow transformer encoder}
\label{\detokenize{doc_contents/how_it_works:flow-transformer-encoder}}
\sphinxAtStartPar
Ottenuti questi vettori rappresentanti gli interi pacchetti,
ripassiamo per un transformer per creare correlazioni tra di loro. Ogni vettore associato ad un pacchetto otterrà una nuova rappresentazione che include le informazioni dei pacchetti circostanti.


\section{Supplement layer (lunghezze)}
\label{\detokenize{doc_contents/how_it_works:supplement-layer-lunghezze}}
\sphinxAtStartPar
Per ottenere gli embedding dei pacchetti, dobbiamo troncare o allungare il pacchetto alla lunghezza desiderata dal modello transformer. Da quel punto in poi (PTE, FTE) i pacchetti trasformati avranno tutti la stessa lunghezza. Per non perdere quell’informazione, prima di fare passare per il transformer estraiamo la lunghezza di tutti i pacchetti. Si crea dunque una sequenze di queste lunghezze che viene data ad un secondo tipo di modello chiamato LSTM (long short term memory), in breve adatto ad individuare relazioni temporali all’interno di sequenze.


\section{Concatenazione e classificazione}
\label{\detokenize{doc_contents/how_it_works:concatenazione-e-classificazione}}\begin{description}
\sphinxlineitem{Alla fine avremo dunque:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Una rappresentazione dei pacchetti di un flow di traffico   trasformata per essere utile alla classificazione.

\item {} 
\sphinxAtStartPar
Una rappresentazione delle lunghezze dei pacchetti, anche   trasformata per esprimere correlazioni tra le lunghezze.

\end{itemize}

\end{description}

\sphinxAtStartPar
Queste due rappresentazioni vengono concatenate e data in input ad una classica rete neurale che predice il tipo di applicazione.


\chapter{Come funziona il modello}
\label{\detokenize{index:come-funziona-il-modello}}\begin{description}
\sphinxlineitem{Il modello è diviso in 4 fasi diverse, che sono in ordine:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Pretrain

\item {} 
\sphinxAtStartPar
Packet tranfsormer encoder (PTE)

\item {} 
\sphinxAtStartPar
Flow transformer encoder (FTE)

\item {} 
\sphinxAtStartPar
Supplement layer (lunghezze pacchetti)

\item {} 
\sphinxAtStartPar
Concatenazione e classificazione

\end{itemize}

\end{description}

\sphinxAtStartPar
Ogni step è spiegato nel dettaglio in {\hyperref[\detokenize{doc_contents/how_it_works::doc}]{\sphinxcrossref{\DUrole{doc}{Fasi del modello}}}}.


\chapter{Come vengono passati i dati al modello}
\label{\detokenize{index:come-vengono-passati-i-dati-al-modello}}
\sphinxAtStartPar
Il dataset che il modello si attende è già diviso in traffici corrispondenti alla stessa applicazione ovvero: ogni sample estratto dal dataset contiene 10 pacchetti (ognuno troncato per avere 400 bytes) dello stesso tipo di traffico.

\sphinxAtStartPar
Questo lavoro di catalogazione non è presente all’interno del codice fornito dagli autori.


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-ref}{genindex}}}

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-ref}{search}}}

\end{itemize}



\renewcommand{\indexname}{Indice}
\printindex
\end{document}